{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing a Model to Analyze Health Vials from Diablo 4 Screenshots\n",
    "\n",
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install torchvision matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Train a model to classify health vials and estimate their fill levels.  \n",
    "<br>\n",
    "### Semi-Supervised Learning\n",
    "Use semi-supervised learning to create a sub-varient of the model to help with the labeleling process\n",
    "<br>\n",
    "#### Load labeled and unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for the datasets\n",
    "base_path = os.path.expanduser('~/Desktop/health-vials/')\n",
    "\n",
    "# Load labeled dataset\n",
    "with open(os.path.join(base_path, 'batch_annotations_full_size(labeled).json'), 'r') as f:\n",
    "    labeled_data = json.load(f)\n",
    "\n",
    "# Load unlabeled dataset\n",
    "with open(os.path.join(base_path, 'batch_annotations_full_size(unlabeled).json'), 'r') as f:\n",
    "    unlabeled_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Custom Dataset\n",
    "class VialDataset(Dataset):\n",
    "    def __init__(self, data, is_labeled=True, transform=None):\n",
    "        self.data = data\n",
    "        self.is_labeled = is_labeled\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image_path = os.path.join(base_path, item['image']) \n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        try:\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image: {image_path}, Error: {str(e)}\")\n",
    "            return None, None, None\n",
    "\n",
    "\n",
    "        if self.is_labeled:\n",
    "            # Return labeled data\n",
    "            fullness = item['attributes']['fullness']\n",
    "            barrier = item['attributes']['barrier']\n",
    "            return image, torch.tensor([fullness], dtype=torch.float32), torch.tensor([int(barrier)], dtype=torch.float32)\n",
    "        \n",
    "        else:\n",
    "            # Return only the image and empty tensors for unlabeled data\n",
    "            return image, torch.zeros(1, dtype=torch.float32), torch.zeros(1, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class VialModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VialModel, self).__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * 118 * 118, 128),  # Adjust to match image size\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fullness_head = nn.Linear(128, 1)  # Regression for fullness\n",
    "        self.barrier_head = nn.Linear(128, 1)  # Classification for barrier\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        fullness = self.fullness_head(x)\n",
    "        barrier = torch.sigmoid(self.barrier_head(x))\n",
    "        return fullness, barrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 16.666851043701172\n",
      "Epoch 2: Loss = 9.9879789352417\n",
      "Epoch 3: Loss = 0.7389234304428101\n",
      "Epoch 4: Loss = 0.4268410801887512\n",
      "Epoch 5: Loss = 1.014037013053894\n",
      "Epoch 6: Loss = 1.373262643814087\n",
      "Epoch 7: Loss = 1.422499656677246\n",
      "Epoch 8: Loss = 0.8485471606254578\n",
      "Epoch 9: Loss = 0.40497589111328125\n",
      "Epoch 10: Loss = 0.667794406414032\n"
     ]
    }
   ],
   "source": [
    "# Pretrain on labeled data\n",
    "labeled_dataset = VialDataset(labeled_data, is_labeled=True, transform=transforms.ToTensor())\n",
    "labeled_loader = DataLoader(labeled_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "model = VialModel()\n",
    "\n",
    "criterion_fullness = nn.MSELoss()  # Loss for regression\n",
    "criterion_barrier = nn.BCELoss()  # Loss for classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop for labeled data\n",
    "model.train()\n",
    "for epoch in range(10):  # Adjust epochs\n",
    "    for images, fullness, barrier in labeled_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred_fullness, pred_barrier = model(images)\n",
    "        loss_fullness = criterion_fullness(pred_fullness, fullness)\n",
    "        loss_barrier = criterion_barrier(pred_barrier, barrier)\n",
    "        loss = loss_fullness + loss_barrier\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}: Loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pseudo-labels for unlabeled data\n",
    "unlabeled_dataset = VialDataset(unlabeled_data, is_labeled=False, transform=transforms.ToTensor())\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "pseudo_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, _, _ in unlabeled_loader:\n",
    "        pred_fullness, pred_barrier = model(images)\n",
    "        for i in range(len(images)):\n",
    "            pseudo_labels.append({\n",
    "                \"image\": unlabeled_data[i]['image'],\n",
    "                \"bounding_box\": unlabeled_data[i]['bounding_box'],\n",
    "                \"attributes\": {\n",
    "                    \"fullness\": float(pred_fullness[i].item()),\n",
    "                    \"barrier\": bool(pred_barrier[i].item() > 0.5)\n",
    "                }\n",
    "            })\n",
    "\n",
    "# Save pseudo-labeled data\n",
    "with open('batch_annotations_pseudo_labeled.json', 'w') as f:\n",
    "    json.dump(pseudo_labels, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets\n",
    "combined_data = labeled_data + pseudo_labels\n",
    "combined_dataset = VialDataset(combined_data, is_labeled=True, transform=transforms.ToTensor())\n",
    "combined_loader = DataLoader(combined_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.3077651858329773\n",
      "Epoch 2: Loss = 0.43559712171554565\n",
      "Epoch 3: Loss = 0.3910799026489258\n",
      "Epoch 4: Loss = 0.1623603254556656\n",
      "Epoch 5: Loss = 0.16949298977851868\n",
      "Epoch 6: Loss = 0.20837630331516266\n",
      "Epoch 7: Loss = 0.1786826252937317\n",
      "Epoch 8: Loss = 0.16844432055950165\n",
      "Epoch 9: Loss = 0.16195252537727356\n",
      "Epoch 10: Loss = 0.13993768393993378\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "for epoch in range(10):  # Adjust epochs\n",
    "    for images, fullness, barrier in combined_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred_fullness, pred_barrier = model(images)\n",
    "        loss_fullness = criterion_fullness(pred_fullness, fullness)\n",
    "        loss_barrier = criterion_barrier(pred_barrier, barrier)\n",
    "        loss = loss_fullness + loss_barrier\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}: Loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: /Users/jpswaynos/Desktop/health-vials/vial_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model_save_path = os.path.join(base_path, 'vial_model.pth')\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f'Model saved at: {model_save_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model for inference\n",
    "def load_model(model_path):\n",
    "    model = VialModel()  # Create a new instance of the model\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model\n",
    "\n",
    "def infer_health_vial(image_path, model):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transforms.ToTensor()(image).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred_fullness, pred_barrier = model(image)\n",
    "        fullness = pred_fullness.item()\n",
    "        barrier = bool(pred_barrier.item() > 0.5)  # Apply sigmoid and threshold\n",
    "        return fullness, barrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import json\n",
    "\n",
    "def display_image(image_path, fullness_value, barrier_present_value):\n",
    "    \"\"\"Display the image in the Jupyter output along with the threshold and barrier values.\"\"\"\n",
    "    img = mpimg.imread(image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')  # Hide axes\n",
    "\n",
    "    # Display threshold and barrier values below the image\n",
    "    plt.title(f'Fullness: {fullness_value}, Barrier Present: {barrier_present_value}')\n",
    "    plt.show()\n",
    "\n",
    "def process_images(image_dir, model, output_file, fullness_threshold=None, barrier_filter=None):\n",
    "    \"\"\"Process images for inference and apply filters based on thresholds.\"\"\"\n",
    "    # List all image files in the directory\n",
    "    image_files = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    # To hold results\n",
    "    results = []\n",
    "    \n",
    "    # Define the bounding box (fixed coordinate values)\n",
    "    bounding_box = {\n",
    "        \"x_min\": 0,\n",
    "        \"y_min\": 0,\n",
    "        \"x_max\": 236,\n",
    "        \"y_max\": 236\n",
    "    }\n",
    "\n",
    "    # Loop through each image file and perform inference\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "\n",
    "        # Perform inference (Assume you have a pre-defined function to call)\n",
    "        fullness, barrier = infer_health_vial(image_path, model)  # Replace this with your model's inference method\n",
    "        \n",
    "        # Construct result for current image\n",
    "        result = {\n",
    "            \"image\": image_file,\n",
    "            \"bounding_box\": bounding_box,\n",
    "            \"attributes\": {\n",
    "                \"fullness\": fullness,\n",
    "                \"barrier\": barrier\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Append result to results list\n",
    "        results.append(result)\n",
    "\n",
    "        # Apply filters\n",
    "        if (fullness_threshold is None or fullness > fullness_threshold) and \\\n",
    "           (barrier_filter is None or barrier == barrier_filter):\n",
    "            # Print the image in the Jupyter output along with threshold and barrier values\n",
    "            display_image(image_path, fullness, barrier)\n",
    "\n",
    "    # Write results to a JSON file\n",
    "    with open(output_file, 'w') as json_file:\n",
    "        json.dump(results, json_file, indent=4)\n",
    "\n",
    "    print(f\"Inference results saved to {output_file}\")\n",
    "\n",
    "#base_path = os.path.join(os.getcwd(), '..') # one directory above the current working directory\n",
    "base_path = os.path.expanduser('~/Desktop/health-vials')\n",
    "\n",
    "# Load the trained model for inference\n",
    "model_path = os.path.join(base_path, 'vial_model.pth')\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Directory containing the images for inference\n",
    "image_dir = os.path.expanduser('~/Desktop/health-vials-eval')\n",
    "output_file = os.path.join(base_path, 'vial_inference_results.json')\n",
    "\n",
    "# Specify filters:\n",
    "# fullness_threshold: Change this to set the minimum fullness to show\n",
    "# barrier_filter:\n",
    "#   None = don't filter on barrier\n",
    "#   True = show images with barrier\n",
    "#   False = show images without barrier\n",
    "fullness_threshold = 0.98 \n",
    "barrier_filter = None\n",
    "\n",
    "# Process images with specified filters\n",
    "process_images(image_dir, model, output_file, fullness_threshold, barrier_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Utilities for preparing the data for training.\n",
    "\n",
    "### Extract Health Vials from Screenshots\n",
    "Using a sample image ```../images/health-vial-mask.1440.png``` this notebook will apply the mask to 1440p screenshots from the game and extract the health vial image.  \n",
    "<br>\n",
    "Additionally, the notebook will resize the image to a size just larger than the extracted health vial, and use a white background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_masked_content(input_image_path, output_image_path, mask_image_path='../images/health-vial-mask.1440.png'):\n",
    "    # Open the input image\n",
    "    img = Image.open(input_image_path).convert(\"RGBA\")\n",
    "    \n",
    "    # Open the mask image and convert to greyscale\n",
    "    mask = Image.open(mask_image_path).convert(\"L\")  # Convert the mask to greyscale\n",
    "\n",
    "    # Create a new image by applying the mask\n",
    "    masked_data = []\n",
    "    data = img.getdata()\n",
    "\n",
    "    for index, item in enumerate(data):\n",
    "        # Get mask pixel value (0: black, 255: white)\n",
    "        mask_value = mask.getpixel((index % img.width, index // img.width))\n",
    "        \n",
    "        # If the mask pixel is white (255), keep the corresponding input image pixel\n",
    "        if mask_value < 255:  # If not white, keep the pixel\n",
    "            masked_data.append(item)  # Keep the original pixel\n",
    "        else:\n",
    "            masked_data.append((0, 0, 0, 0))  # Fully transparent pixel\n",
    "\n",
    "    # Create a new image with the masked content\n",
    "    masked_image = Image.new(\"RGBA\", img.size)\n",
    "    masked_image.putdata(masked_data)\n",
    "\n",
    "    # Find the bounding box of the non-transparent pixels\n",
    "    bbox = masked_image.getbbox()\n",
    "\n",
    "    # Crop the image to the bounding box\n",
    "    if bbox:\n",
    "        cropped_image = masked_image.crop(bbox)\n",
    "\n",
    "        # Create a new transparent image for resizing\n",
    "        max_size = max(cropped_image.size)\n",
    "        new_image = Image.new(\"RGBA\", (max_size, max_size), (0, 0, 0, 0))  # Transparent background\n",
    "\n",
    "        # Paste cropped image onto the center of the new transparent image\n",
    "        new_image.paste(cropped_image, ((max_size - cropped_image.width) // 2, (max_size - cropped_image.height) // 2))\n",
    "\n",
    "        # Save the result\n",
    "        new_image.save(output_image_path, format=\"PNG\")\n",
    "    else:\n",
    "        print(\"No visible content to extract.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directories\n",
    "input_base_dir = os.path.expanduser('~/Desktop/Diablo 4 Captures')\n",
    "output_base_dir = os.path.expanduser('~/Desktop/health-vials-eval')\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "# Get a list of all files in the input directory\n",
    "all_files = os.listdir(input_base_dir)\n",
    "\n",
    "# Filter to get only PNG files\n",
    "png_image_paths = [os.path.join(input_base_dir, f) for f in all_files if f.endswith('.png')]\n",
    "\n",
    "# Process each PNG image\n",
    "for input_image_path in png_image_paths:\n",
    "    # Construct the output image path\n",
    "    output_image_path = os.path.join(output_base_dir, f'extracted_{os.path.basename(input_image_path)}')\n",
    "    \n",
    "    extract_masked_content(input_image_path, output_image_path)\n",
    "    print(f'Processing {input_image_path} => {output_image_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop and Resize Screenshots\n",
    "Remove top and bottom 105px and resize the screenshot to 2560x1440."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your desktop\n",
    "images_path = os.path.expanduser(\"~/Desktop/Diablo 4 Captures\")\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for file_name in os.listdir(images_path):\n",
    "    if file_name.endswith(\".png\"):  # Process only PNG files\n",
    "        file_path = os.path.join(images_path, file_name)\n",
    "        \n",
    "        # Open the image\n",
    "        with Image.open(file_path) as img:\n",
    "            width, height = img.size\n",
    "            \n",
    "            # Check if the image dimensions are 2560 x 1440\n",
    "            if (width, height) > (2560, 1440):\n",
    "                \n",
    "                # Crop the image to remove 105px from top and bottom\n",
    "                cropped_img = img.crop((0, 105, width, height - 105))\n",
    "\n",
    "                # Resize the cropped image to 2560x1440 if it isn't already\n",
    "                resized_img = cropped_img.resize((2560, 1440), Image.LANCZOS)\n",
    "                \n",
    "                # Save the resized image (overwrite the original file)\n",
    "                cropped_img.save(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
