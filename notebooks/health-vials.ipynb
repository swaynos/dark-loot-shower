{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing a Model to Analyze Health Vials from Diablo 4 Screenshots\n",
    "\n",
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install torchvision matplotlib openpyxl Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import base64\n",
    "import html\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from openpyxl import Workbook, load_workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image as PILImage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Train a model to classify health vials and estimate their fill levels.  \n",
    "<br>\n",
    "### Semi-Supervised Learning\n",
    "Use semi-supervised learning to create a sub-varient of the model to help with the labeleling process\n",
    "<br>\n",
    "#### Load labeled and unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for the datasets\n",
    "base_path = os.path.expanduser('~/Desktop/health-vials/')\n",
    "\n",
    "# Load labeled dataset\n",
    "with open(os.path.join(base_path, 'batch_annotations_full_size(labeled).json'), 'r') as f:\n",
    "    labeled_data = json.load(f)\n",
    "\n",
    "# Load unlabeled dataset\n",
    "with open(os.path.join(base_path, 'batch_annotations_full_size(unlabeled).json'), 'r') as f:\n",
    "    unlabeled_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Custom Dataset\n",
    "class VialDataset(Dataset):\n",
    "    def __init__(self, data, is_labeled=True, transform=None):\n",
    "        self.data = data\n",
    "        self.is_labeled = is_labeled\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image_path = os.path.join(base_path, item['image']) \n",
    "        image = PLImage.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        try:\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image: {image_path}, Error: {str(e)}\")\n",
    "            return None, None, None\n",
    "\n",
    "\n",
    "        if self.is_labeled:\n",
    "            # Return labeled data\n",
    "            fullness = item['attributes']['fullness']\n",
    "            barrier = item['attributes']['barrier']\n",
    "            return image, torch.tensor([fullness], dtype=torch.float32), torch.tensor([int(barrier)], dtype=torch.float32)\n",
    "        \n",
    "        else:\n",
    "            # Return only the image and empty tensors for unlabeled data\n",
    "            return image, torch.zeros(1, dtype=torch.float32), torch.zeros(1, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class VialModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VialModel, self).__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * 118 * 118, 128),  # Adjust to match image size\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fullness_head = nn.Linear(128, 1)  # Regression for fullness\n",
    "        self.barrier_head = nn.Linear(128, 1)  # Classification for barrier\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        fullness = self.fullness_head(x)\n",
    "        barrier = torch.sigmoid(self.barrier_head(x))\n",
    "        return fullness, barrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain on labeled data\n",
    "labeled_dataset = VialDataset(labeled_data, is_labeled=True, transform=transforms.ToTensor())\n",
    "labeled_loader = DataLoader(labeled_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "model = VialModel()\n",
    "\n",
    "criterion_fullness = nn.MSELoss()  # Loss for regression\n",
    "criterion_barrier = nn.BCELoss()  # Loss for classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop for labeled data\n",
    "model.train()\n",
    "for epoch in range(10):  # Adjust epochs\n",
    "    for images, fullness, barrier in labeled_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred_fullness, pred_barrier = model(images)\n",
    "        loss_fullness = criterion_fullness(pred_fullness, fullness)\n",
    "        loss_barrier = criterion_barrier(pred_barrier, barrier)\n",
    "        loss = loss_fullness + loss_barrier\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}: Loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pseudo-labels for unlabeled data\n",
    "unlabeled_dataset = VialDataset(unlabeled_data, is_labeled=False, transform=transforms.ToTensor())\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "pseudo_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, _, _ in unlabeled_loader:\n",
    "        pred_fullness, pred_barrier = model(images)\n",
    "        for i in range(len(images)):\n",
    "            pseudo_labels.append({\n",
    "                \"image\": unlabeled_data[i]['image'],\n",
    "                \"bounding_box\": unlabeled_data[i]['bounding_box'],\n",
    "                \"attributes\": {\n",
    "                    \"fullness\": float(pred_fullness[i].item()),\n",
    "                    \"barrier\": bool(pred_barrier[i].item() > 0.5)\n",
    "                }\n",
    "            })\n",
    "\n",
    "# Save pseudo-labeled data\n",
    "with open('batch_annotations_pseudo_labeled.json', 'w') as f:\n",
    "    json.dump(pseudo_labels, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets\n",
    "combined_data = labeled_data + pseudo_labels\n",
    "combined_dataset = VialDataset(combined_data, is_labeled=True, transform=transforms.ToTensor())\n",
    "combined_loader = DataLoader(combined_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "for epoch in range(10):  # Adjust epochs\n",
    "    for images, fullness, barrier in combined_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred_fullness, pred_barrier = model(images)\n",
    "        loss_fullness = criterion_fullness(pred_fullness, fullness)\n",
    "        loss_barrier = criterion_barrier(pred_barrier, barrier)\n",
    "        loss = loss_fullness + loss_barrier\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}: Loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model_save_path = os.path.join(base_path, 'vial_model.pth')\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f'Model saved at: {model_save_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "In this section, we will utilize the trained model to perform inference on health vial images extracted from screenshots. The goal is to predict two main attributes for each health vial:\n",
    "- **Fullness**: A continuous value indicating how full the vial is (ranging from 0.0 to 1.0).\n",
    "- **Barrier Presence**: A binary value indicating whether a barrier is present (True or False).\n",
    "\n",
    "We will follow these steps:\n",
    "\n",
    "1. **Load the Trained Model**: \n",
    "   - A pre-trained model will be loaded from a specified file path to make predictions on unseen images.\n",
    "\n",
    "2. **Image Processing for Inference**: \n",
    "   - The function `process_images` is defined to process all images within a directory. For each image, the model will be queried to get the predictions for fullness and barrier presence.\n",
    "   - It will also format the output by displaying each image alongside its respective predictions. \n",
    "\n",
    "3. **Display Predictions**: \n",
    "   - The `display_image` function is utilized to visually present each vial image together with formatted fullness and barrier information, making it easier to review the modelâ€™s output.\n",
    "\n",
    "4. **Results Compilation**: \n",
    "   - An organized list of results is compiled, containing the image name, bounding box information (fixed values in this case), and predicted attributes (fullness and barrier presence).\n",
    "  \n",
    "5. **Exporting Results**: \n",
    "   - Finally, results from the inference phase are saved into an Excel file for human review. The results will include the images and attributes in an easy-to-read format.\n",
    "\n",
    "This process will help in evaluating the model's performance and the accuracy of its predictions on the health vial images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model for inference\n",
    "def load_model(model_path):\n",
    "    model = VialModel()  # Create a new instance of the model\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model\n",
    "\n",
    "def infer_health_vial(image_path, model):\n",
    "    # Load and preprocess the image\n",
    "    image = PLImage.open(image_path).convert(\"RGB\")\n",
    "    image = transforms.ToTensor()(image).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred_fullness, pred_barrier = model(image)\n",
    "        fullness = pred_fullness.item()\n",
    "        barrier = bool(pred_barrier.item() > 0.5)  # Apply sigmoid and threshold\n",
    "        return fullness, barrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image_path, fullness_value, barrier_present_value):\n",
    "    \"\"\"Display the image in the Jupyter output along with the threshold and barrier values.\"\"\"\n",
    "    img = mpimg.imread(image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')  # Hide axes\n",
    "\n",
    "    # Display threshold and barrier values below the image\n",
    "    plt.title(f'Fullness: {fullness_value}, Barrier Present: {barrier_present_value}')\n",
    "    plt.show()\n",
    "\n",
    "def process_images(image_dir, model):\n",
    "    \"\"\"Process images for inference and apply filters based on thresholds.\"\"\"\n",
    "    # List all image files in the directory\n",
    "    image_files = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    # To hold results\n",
    "    results = []\n",
    "    \n",
    "    # Define the bounding box (fixed coordinate values)\n",
    "    bounding_box = {\n",
    "        \"x_min\": 0,\n",
    "        \"y_min\": 0,\n",
    "        \"x_max\": 236,\n",
    "        \"y_max\": 236\n",
    "    }\n",
    "\n",
    "    # Loop through each image file and perform inference\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "\n",
    "        # Perform inference\n",
    "        fullness, barrier = infer_health_vial(image_path, model)\n",
    "\n",
    "        # Cleanup the fullness\n",
    "        # 1. Round fullness to the nearest 0.05.\n",
    "        # 2. Apply a ceiling to ensure the fullness does not exceed 1.0.\n",
    "        fullness = round(min(fullness, 1.0) * 20) / 20 \n",
    "\n",
    "        # Display the image\n",
    "        display_image(image_path, fullness, barrier)\n",
    "\n",
    "        # Construct result for current image\n",
    "        result = {\n",
    "            \"image\": image_file,\n",
    "            \"bounding_box\": bounding_box,\n",
    "            \"attributes\": {\n",
    "                \"fullness\": fullness,\n",
    "                \"barrier\": barrier\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Append result to results list\n",
    "        results.append(result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_path = os.path.join(os.getcwd(), '..') # one directory above the current working directory\n",
    "base_path = os.path.expanduser('~/Desktop/health-vials')\n",
    "\n",
    "# Load the trained model for inference\n",
    "model_path = os.path.join(base_path, 'vial_model.pth')\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Directory containing the images for inference\n",
    "image_dir = os.path.expanduser('~/Desktop/health-vials-eval')\n",
    "\n",
    "# Process images with specified filters\n",
    "results = process_images(image_dir, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Inference Results for Human Review\n",
    "\n",
    "In this section, we aim to compile the results from our inference process into a format that is easy for humans to review. The output will be structured in a way that displays relevant image attributes alongside the images themselves.\n",
    "\n",
    "Key steps involved in this process include:\n",
    "\n",
    "1. **Exporting Results to Excel**: \n",
    "   - The `export_results_excel_with_images` function creates a new Excel workbook and populates it with the inference results. Each entry includes the image, its name, the predicted fullness, and the barrier presence. This results in a spreadsheet that is user-friendly and enables easy sorting or filtering for manual review.\n",
    "\n",
    "2. **Finalizing the Output**: \n",
    "   - Lastly, the Excel workbook is saved to a specified output path. This file serves as a detailed report for human reviewers, allowing for assessment of both the images and their predicted attributes.\n",
    "\n",
    "By executing these steps, we ensure that the results from the model's predictions are easily accessible for validation, further analysis, and enhancement of the machine learning framework as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results_excel_with_images(results, output_excel_path):\n",
    "    \"\"\"Export processed results to an Excel file with images.\"\"\"\n",
    "    # Create a new Excel workbook and a worksheet\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = \"Health Vials Results\"\n",
    "\n",
    "    # Write header\n",
    "    ws.append([\"Image\", \"Image Name\", \"Fullness\", \"Barrier\"])\n",
    "\n",
    "    # Loop through the results and add to Excel\n",
    "    for result in results:\n",
    "        # Add fullness and barrier values\n",
    "        fullness = result['attributes']['fullness']\n",
    "        barrier = result['attributes']['barrier']\n",
    "        image_name = result['image']\n",
    "\n",
    "        # Add the image\n",
    "        image_path = os.path.join(image_dir, image_name)\n",
    "        img = Image(image_path)  # Create an Image object\n",
    "        img.width = 100  # Adjust the width (optional)\n",
    "        img.height = 100  # Adjust the height (optional)\n",
    "        \n",
    "        # Append a new row in the sheet and insert image in the first column\n",
    "        ws.append([None, image_name, fullness, barrier])  # Append empty cell for image\n",
    "        ws.add_image(img, ws.cell(row=ws.max_row, column=1).coordinate) # Add image in the first column\n",
    "        \n",
    "\n",
    "    # Save the workbook\n",
    "    wb.save(output_excel_path)\n",
    "\n",
    "# Assuming 'results' is a list of dictionaries containing the needed information\n",
    "output_path = os.path.expanduser(\"~/Desktop/health_vials_results.xlsx\")\n",
    "export_results_excel_with_images(results, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Back from Excel\n",
    "\n",
    "In this section, we will read the inference results previously exported to an Excel file and reconstruct them for further use. \n",
    "- **Filtering Results**: Results are appended to a list only if the 'purge' status is `False`, ensuring that only valid entries are retained.\n",
    "- **Constructing JSON Objects**: Each valid entry is transformed into a JSON object, with the image name, bounding box, and attributes (fullness and barrier) included.\n",
    "- **Saving**: The json objects are saved to a file for further use in retraining the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the static bounding box\n",
    "bounding_box = {\n",
    "    \"x_min\": 0,\n",
    "    \"y_min\": 0,\n",
    "    \"x_max\": 236,\n",
    "    \"y_max\": 236\n",
    "}\n",
    "\n",
    "def read_excel_and_construct_json(excel_path):\n",
    "    # Load the workbook and the specific sheet\n",
    "    workbook = load_workbook(excel_path)\n",
    "    sheet = workbook.active\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Iterate through the rows of the sheet, starting from row 2 to skip the header\n",
    "    for row in sheet.iter_rows(min_row=2, values_only=True):\n",
    "        image, image_name, fullness, barrier, purge = row\n",
    "        \n",
    "        # Check if purge is FALSE\n",
    "        if purge is False:\n",
    "            result = {\n",
    "                \"image\": image_name,\n",
    "                \"bounding_box\": bounding_box,\n",
    "                \"attributes\": {\n",
    "                    \"fullness\": fullness,\n",
    "                    \"barrier\": barrier,\n",
    "                }\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Specify the path to your Excel file\n",
    "excel_path = os.path.expanduser(\"~/Desktop/health_vials_results.xlsx\")\n",
    "reconstructed_results = read_excel_and_construct_json(excel_path)\n",
    "\n",
    "# Save results to a JSON file\n",
    "output_json_path = os.path.expanduser(\"~/Desktop/health-vials/vial_inference_results.json\")\n",
    "with open(output_json_path, 'w') as json_file:\n",
    "    json.dump(reconstructed_results, json_file, indent=4)\n",
    "\n",
    "print(f\"Results saved to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Utilities\n",
    "\n",
    "This section includes additional tools and functions designed to facilitate the preparation and processing of data required for training our health vial analysis model. These utilities streamline the workflow, ensuring that input images are correctly formatted and suitable for model training.\n",
    "\n",
    "### Extract Health Vials from Screenshots\n",
    "\n",
    "In this subsection, we will utilize a mask to automate the extraction of health vial images from high-resolution screenshots (1440p) of Diablo 4.\n",
    "\n",
    "Key steps involved include:\n",
    "\n",
    "1. **Applying the Mask**: \n",
    "   - The provided mask image (`../images/health-vial-mask.1440.png`) is used to identify and isolate health vials within the screenshots. This mask effectively filters the relevant portions of the images, ensuring that only the health vial contents are extracted for analysis.\n",
    "\n",
    "2. **Image Resizing**: \n",
    "   - Once the health vial is extracted, the resulting image is resized slightly larger than the original dimensions, which helps maintain quality and clarity. A white background is used to ensure that the health vial images stand out, which is especially useful for subsequent training phases.\n",
    "\n",
    "3. **Maintaining Consistency**:\n",
    "   - The extraction process ensures that all health vials are represented with a uniform size and background. This consistency is crucial for training the model, as it allows the algorithm to focus on learning the features and patterns typical of health vials without variations due to size or background distractions.\n",
    "\n",
    "By performing these operations, we enhance the overall quality and usability of the image data, thereby improving the robustness of the machine learning model during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_masked_content(input_image_path, output_image_path, mask_image_path='../images/health-vial-mask.1440.png'):\n",
    "    # Open the input image\n",
    "    img = Image.open(input_image_path).convert(\"RGBA\")\n",
    "    \n",
    "    # Open the mask image and convert to greyscale\n",
    "    mask = Image.open(mask_image_path).convert(\"L\")  # Convert the mask to greyscale\n",
    "\n",
    "    # Create a new image by applying the mask\n",
    "    masked_data = []\n",
    "    data = img.getdata()\n",
    "\n",
    "    for index, item in enumerate(data):\n",
    "        # Get mask pixel value (0: black, 255: white)\n",
    "        mask_value = mask.getpixel((index % img.width, index // img.width))\n",
    "        \n",
    "        # If the mask pixel is white (255), keep the corresponding input image pixel\n",
    "        if mask_value < 255:  # If not white, keep the pixel\n",
    "            masked_data.append(item)  # Keep the original pixel\n",
    "        else:\n",
    "            masked_data.append((0, 0, 0, 0))  # Fully transparent pixel\n",
    "\n",
    "    # Create a new image with the masked content\n",
    "    masked_image = Image.new(\"RGBA\", img.size)\n",
    "    masked_image.putdata(masked_data)\n",
    "\n",
    "    # Find the bounding box of the non-transparent pixels\n",
    "    bbox = masked_image.getbbox()\n",
    "\n",
    "    # Crop the image to the bounding box\n",
    "    if bbox:\n",
    "        cropped_image = masked_image.crop(bbox)\n",
    "\n",
    "        # Create a new transparent image for resizing\n",
    "        max_size = max(cropped_image.size)\n",
    "        new_image = Image.new(\"RGBA\", (max_size, max_size), (0, 0, 0, 0))  # Transparent background\n",
    "\n",
    "        # Paste cropped image onto the center of the new transparent image\n",
    "        new_image.paste(cropped_image, ((max_size - cropped_image.width) // 2, (max_size - cropped_image.height) // 2))\n",
    "\n",
    "        # Save the result\n",
    "        new_image.save(output_image_path, format=\"PNG\")\n",
    "    else:\n",
    "        print(\"No visible content to extract.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directories\n",
    "input_base_dir = os.path.expanduser('~/Desktop/Diablo 4 Captures')\n",
    "output_base_dir = os.path.expanduser('~/Desktop/health-vials-eval')\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "# Get a list of all files in the input directory\n",
    "all_files = os.listdir(input_base_dir)\n",
    "\n",
    "# Filter to get only PNG files\n",
    "png_image_paths = [os.path.join(input_base_dir, f) for f in all_files if f.endswith('.png')]\n",
    "\n",
    "# Process each PNG image\n",
    "for input_image_path in png_image_paths:\n",
    "    # Construct the output image path\n",
    "    output_image_path = os.path.join(output_base_dir, f'extracted_{os.path.basename(input_image_path)}')\n",
    "    \n",
    "    extract_masked_content(input_image_path, output_image_path)\n",
    "    print(f'Processing {input_image_path} => {output_image_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop and Resize Screenshots\n",
    "\n",
    "In this subsection, we will preprocess screenshots captured at a resolution of 3360x2100, which include 105 pixels of letterboxing at the top and bottom margins. Please note that this process can be slow, especially when handling large numbers of images.\n",
    "\n",
    "1. **Crop the Images**: \n",
    "   - We will remove 105 pixels from both the top and bottom of each screenshot to eliminate unwanted interface elements. This step ensures that the analysis focuses solely on the relevant content of the images.\n",
    "\n",
    "2. **Resize the Images**: \n",
    "   - After cropping, the images will be resized to a standardized resolution of 2560x1440 pixels. This consistency is vital for the modelâ€™s training, as it requires uniform input sizes across all images.\n",
    "\n",
    "These preprocessing steps enhance the quality of the input images from this specific setup, leading to more efficient and effective model training and inference, albeit potentially at the cost of processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your desktop\n",
    "images_path = os.path.expanduser(\"~/Desktop\")\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for file_name in os.listdir(images_path):\n",
    "    if file_name.endswith(\".png\"):  # Process only PNG files\n",
    "        file_path = os.path.join(images_path, file_name)\n",
    "        \n",
    "        # Open the image\n",
    "        with PILImage.open(file_path) as img:\n",
    "            width, height = img.size\n",
    "            \n",
    "            # Check if the image dimensions are 3360 x 2100\n",
    "            if (width, height) == (3360, 2100):\n",
    "                \n",
    "                # Crop the image to remove 105px from top and bottom\n",
    "                cropped_img = img.crop((0, 105, width, height - 105))\n",
    "\n",
    "                # Resize the cropped image to 2560x1440 if it isn't already\n",
    "                resized_img = cropped_img.resize((2560, 1440), PILImage.LANCZOS)\n",
    "                \n",
    "                # Save the resized image (overwrite the original file)\n",
    "                cropped_img.save(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
